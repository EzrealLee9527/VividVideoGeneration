{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/work/animate_based_ap_ctrl/animatediff/magic_animate/pipeline.py:44: FutureWarning: Importing `DiffusionPipeline` or `ImagePipelineOutput` from diffusers.pipeline_utils is deprecated. Please import from diffusers.pipelines.pipeline_utils instead.\n",
      "  from diffusers.pipeline_utils import DiffusionPipeline\n",
      "/home/juneliang/.local/lib/python3.8/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/juneliang/.local/lib/python3.8/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/juneliang/.local/lib/python3.8/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/juneliang/.local/lib/python3.8/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/juneliang/.local/lib/python3.8/site-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import logging\n",
    "import inspect\n",
    "import argparse\n",
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from safetensors import safe_open\n",
    "from typing import Dict, Optional, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import numpy as np\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "from diffusers.models import UNet2DConditionModel\n",
    "from diffusers.pipelines import StableDiffusionPipeline\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "import transformers\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "# from ip_adapter import IPAdapterFull\n",
    "\n",
    "# from animatediff.data.dataset import WebVid10M, PexelsDataset\n",
    "from animatediff.utils.util import save_videos_grid, pad_image\n",
    "from accelerate import Accelerator\n",
    "from einops import repeat\n",
    "from animate import MagicAnimate\n",
    "from animatediff.magic_animate.controlnet import ControlNetModel\n",
    "import importlib\n",
    "from animatediff.data.dataset import WebVid10M, PexelsDataset\n",
    "import webdataset as wds\n",
    "from animatediff.data.dataset_wds import S3VideosIterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/data/work/animate_based_ap_ctrl/configs/training/train_stage1_w_imageencoder_celebv_nocenterface_cfg_train_zeroembed_plusbank.yaml\"\n",
    "config = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = S3VideosIterableDataset(**config.validation_data['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = wds.WebLoader(\n",
    "            eval_dataset, \n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            collate_fn = None,\n",
    "        ).with_length(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/data/work/animate_based_ap_ctrl/debug.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsp/data/work/animate_based_ap_ctrl/debug.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m eval_dataloader[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:53\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T_co:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ce531f1f6b4c5daceda443310aed7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[-0.2477, -0.2639, -0.2729,  ...,  0.0520,  0.0586,  0.0601],\n",
      "          [-0.2283, -0.2432, -0.2541,  ...,  0.0510,  0.0568,  0.0694],\n",
      "          [-0.1867, -0.2030, -0.2243,  ...,  0.0529,  0.0577,  0.0651],\n",
      "          ...,\n",
      "          [ 0.3725,  0.3725,  0.3725,  ...,  0.3950,  0.3941,  0.3920],\n",
      "          [ 0.3725,  0.3725,  0.3725,  ...,  0.3950,  0.3941,  0.3889],\n",
      "          [ 0.3725,  0.3725,  0.3725,  ...,  0.3950,  0.3941,  0.3889]],\n",
      "\n",
      "         [[ 0.1693,  0.1675,  0.1652,  ...,  0.3187,  0.3253,  0.3267],\n",
      "          [ 0.1848,  0.1862,  0.1820,  ...,  0.3176,  0.3235,  0.3361],\n",
      "          [ 0.2153,  0.2205,  0.2060,  ...,  0.3196,  0.3243,  0.3317],\n",
      "          ...,\n",
      "          [ 0.5890,  0.5922,  0.5922,  ...,  0.4902,  0.4960,  0.4940],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.4902,  0.4960,  0.4909],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.4902,  0.4960,  0.4909]],\n",
      "\n",
      "         [[ 0.4438,  0.4420,  0.4364,  ...,  0.4285,  0.4351,  0.4365],\n",
      "          [ 0.4513,  0.4545,  0.4479,  ...,  0.4275,  0.4333,  0.4459],\n",
      "          [ 0.4591,  0.4715,  0.4570,  ...,  0.4294,  0.4342,  0.4415],\n",
      "          ...,\n",
      "          [ 0.6533,  0.6471,  0.6471,  ...,  0.4824,  0.4882,  0.4861],\n",
      "          [ 0.6471,  0.6471,  0.6471,  ...,  0.4824,  0.4882,  0.4830],\n",
      "          [ 0.6471,  0.6471,  0.6471,  ...,  0.4824,  0.4882,  0.4830]]]]), 'pixel_values_ref_img': tensor([[[-0.1459, -0.1577, -0.1431,  ...,  0.1294,  0.1294,  0.1287],\n",
      "         [-0.1439, -0.1439, -0.1419,  ...,  0.1318,  0.1318,  0.1192],\n",
      "         [-0.1353, -0.1350, -0.1318,  ...,  0.1451,  0.1451,  0.1413],\n",
      "         ...,\n",
      "         [ 0.4039,  0.4039,  0.4039,  ...,  0.7255,  0.7255,  0.7255],\n",
      "         [ 0.3973,  0.3973,  0.3973,  ...,  0.7255,  0.7255,  0.7255],\n",
      "         [ 0.3961,  0.3961,  0.3961,  ...,  0.7255,  0.7255,  0.7255]],\n",
      "\n",
      "        [[ 0.2541,  0.2423,  0.2569,  ...,  0.4039,  0.4039,  0.4032],\n",
      "         [ 0.2561,  0.2563,  0.2593,  ...,  0.4027,  0.4027,  0.3901],\n",
      "         [ 0.2647,  0.2662,  0.2761,  ...,  0.3961,  0.3961,  0.3923],\n",
      "         ...,\n",
      "         [ 0.6235,  0.6235,  0.6235,  ...,  0.8118,  0.8118,  0.8118],\n",
      "         [ 0.6169,  0.6169,  0.6169,  ...,  0.8118,  0.8118,  0.8118],\n",
      "         [ 0.6157,  0.6157,  0.6157,  ...,  0.8118,  0.8118,  0.8118]],\n",
      "\n",
      "        [[ 0.4973,  0.4854,  0.5000,  ...,  0.4745,  0.4878,  0.4894],\n",
      "         [ 0.4992,  0.4989,  0.4988,  ...,  0.4769,  0.4882,  0.4776],\n",
      "         [ 0.5079,  0.5058,  0.4957,  ...,  0.4902,  0.4902,  0.4864],\n",
      "         ...,\n",
      "         [ 0.6784,  0.6784,  0.6784,  ...,  0.8039,  0.8039,  0.8039],\n",
      "         [ 0.6718,  0.6718,  0.6718,  ...,  0.8039,  0.8039,  0.8039],\n",
      "         [ 0.6706,  0.6706,  0.6706,  ...,  0.8039,  0.8039,  0.8039]]]), 'drop_image_embeds': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juneliang/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for idx,data in tqdm(enumerate(eval_dataset)):\n",
    "    print(data['pixel_values'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
